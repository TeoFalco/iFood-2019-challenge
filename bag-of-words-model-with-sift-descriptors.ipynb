{"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5408,"databundleVersionId":38263,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction ##\nThe idea here is to use a bag of visual words model to classify the different images. We will use SIFT algorithm to extract the keypoints of each image and create the bag of words.<br>\nMore information about this method can be found here:<br><ul>\n<li>http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf</li>\n<li>https://www.youtube.com/watch?v=iGZpJZhqEME</li>\n\nSome part of this script are inside function, it's just a way to avoid error when I will publish this notebook. If you want to use this script, just remove line starting by \"def ...\".","metadata":{"_cell_guid":"f7c93524-2268-d46a-3386-321b5f7f3309"}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport pandas as pd\nimport csv\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.neural_network import MLPClassifier\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\n\nimport os\nfrom PIL import Image\nimport cv2","metadata":{"_cell_guid":"4fd239a0-39fd-7bc4-4ed3-d3bf87c21305","execution":{"iopub.status.busy":"2024-06-09T13:52:48.603556Z","iopub.execute_input":"2024-06-09T13:52:48.605424Z","iopub.status.idle":"2024-06-09T13:52:48.615084Z","shell.execute_reply.started":"2024-06-09T13:52:48.605359Z","shell.execute_reply":"2024-06-09T13:52:48.613505Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Setup Gdrive file download extention \n!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:52:48.767491Z","iopub.execute_input":"2024-06-09T13:52:48.768006Z","iopub.status.idle":"2024-06-09T13:53:34.069283Z","shell.execute_reply.started":"2024-06-09T13:52:48.767967Z","shell.execute_reply":"2024-06-09T13:53:34.067646Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Channels:\n - rapidsai\n - nvidia\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): / Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf505cf4640>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /nvidia/linux-64/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf506a6b9a0>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /conda-forge/noarch/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf506a9bbb0>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /rapidsai/linux-64/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf505cf47c0>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /rapidsai/noarch/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf506a9b910>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /conda-forge/linux-64/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf506a9b5b0>: Failed to resolve 'conda.anaconda.org' ([Errno -3] Temporary failure in name resolution)\")': /nvidia/noarch/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf505cf4400>: Failed to resolve 'repo.anaconda.com' ([Errno -3] Temporary failure in name resolution)\")': /pkgs/main/noarch/repodata.json.zst\n\nRetrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bf505cf50c0>: Failed to resolve 'repo.anaconda.com' ([Errno -3] Temporary failure in name resolution)\")': /pkgs/main/linux-64/repodata.json.zst\n\n- ^C\n| ","output_type":"stream"}]},{"cell_type":"code","source":"!gdown  1zbmXm-A-wAKoPuQ_fV1tfMi2KXsOKRw4\n!gdown  1qxYTECYa6_uHSW4V61WzX54BgcZ9hCKe\n!gdown  1fX_2IDL06OGvLQtnhvFzBy_BDsdaa2Dz","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:36:05.137158Z","iopub.execute_input":"2024-06-09T13:36:05.137873Z","iopub.status.idle":"2024-06-09T13:36:08.507432Z","shell.execute_reply.started":"2024-06-09T13:36:05.137810Z","shell.execute_reply":"2024-06-09T13:36:08.505765Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/bin/bash: gdown: command not found\n/bin/bash: gdown: command not found\n/bin/bash: gdown: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"#extract\n!tar -xf /kaggle/working/annot.tar\n!tar -xf /kaggle/working/train.tar\n!tar -xf /kaggle/working/val.tar","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:36:08.510650Z","iopub.execute_input":"2024-06-09T13:36:08.511115Z","iopub.status.idle":"2024-06-09T13:36:11.847676Z","shell.execute_reply.started":"2024-06-09T13:36:08.511073Z","shell.execute_reply":"2024-06-09T13:36:11.846000Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tar: /kaggle/working/annot.tar: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\ntar: /kaggle/working/train.tar: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\ntar: /kaggle/working/val.tar: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dir = '/kaggle/working/train_set'\ntest_dir = '/kaggle/working/val_set'","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:36:11.849463Z","iopub.execute_input":"2024-06-09T13:36:11.849879Z","iopub.status.idle":"2024-06-09T13:36:11.856848Z","shell.execute_reply.started":"2024-06-09T13:36:11.849842Z","shell.execute_reply":"2024-06-09T13:36:11.855103Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/train_info.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T13:36:11.858925Z","iopub.execute_input":"2024-06-09T13:36:11.859336Z","iopub.status.idle":"2024-06-09T13:36:12.995163Z","shell.execute_reply.started":"2024-06-09T13:36:11.859304Z","shell.execute_reply":"2024-06-09T13:36:12.993185Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/train_info.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/train_info.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/train_info.csv'","output_type":"error"}]},{"cell_type":"markdown","source":"To do it, we will use OpenCV (cv2) library to extract keypoints with SIFT algorithm.","metadata":{"_cell_guid":"bd3316dd-1e7c-5acc-8d86-48e6704b42ee"}},{"cell_type":"markdown","source":"## Extract keypoints from each image ##","metadata":{"_cell_guid":"0849ae08-ff33-bd8c-abb2-db52e1441eee"}},{"cell_type":"code","source":"img_path = '../input/images/'\ntrain = pd.read_csv('../input/train.csv')\nspecies = train.species.sort_values().unique()\n\ndico = []\n\nfor leaf in train.id:\n    img = cv2.imread(img_path + str(leaf) + \".jpg\")\n    kp, des = sift.detectAndCompute(img, None)\n\n    for d in des:\n        dico.append(d)","metadata":{"_cell_guid":"cd6c53d2-0822-fd32-7a20-447d485ebafe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clustering  ##\nWe now have an array with a huge number of descriptors. We cannot use all of them to create or model so we need to cluster them. A rule-of-thumb is to create k centers with k = number of categories * 10 (in our case, it's 990).","metadata":{"_cell_guid":"58b3257e-6916-0ed0-bdb5-9c23dd6e0ab8"}},{"cell_type":"code","source":"k = np.size(species) * 10\n\nbatch_size = np.size(os.listdir(img_path)) * 3\nkmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, verbose=1).fit(dico)","metadata":{"_cell_guid":"ca3fc57e-e7f1-1d10-70fb-0b521b2da700"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use MiniBatchKMeans to avoid Memory Error.","metadata":{"_cell_guid":"758507e5-cf26-03f6-3fab-cf55f83accfb"}},{"cell_type":"markdown","source":"## Creation of the histograms ##\nTo create our each image by a histogram. We will create a vector of k value for each image. For each keypoints in an image, we will find the nearest center and increase by one its value.","metadata":{"_cell_guid":"5d9b232c-af09-1673-e3f4-3869ea491321"}},{"cell_type":"code","source":"kmeans.verbose = False\n\nhisto_list = []\n\nfor leaf in train.id:\n    img = cv2.imread(img_path + str(leaf) + \".jpg\")\n    kp, des = sift.detectAndCompute(img, None)\n\n    histo = np.zeros(k)\n    nkp = np.size(kp)\n\n    for d in des:\n        idx = kmeans.predict([d])\n        histo[idx] += 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n\n    histo_list.append(histo)","metadata":{"_cell_guid":"47eb763e-b505-1599-2f83-50d013339a0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training of the neural network ##","metadata":{"_cell_guid":"5197a0cb-cb3c-0968-4521-32d6b9559a79"}},{"cell_type":"code","source":"X = np.array(histo_list)\nY = []\n\n# It's a way to convert species name into an integer\nfor s in train.species:\n    Y.append(np.min(np.nonzero(species == s)))\n\nmlp = MLPClassifier(verbose=True, max_iter=600000)\nmlp.fit(X, Y)","metadata":{"_cell_guid":"88e782f5-8f6d-ac67-5e9e-d30eb3b99c26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions ##","metadata":{"_cell_guid":"4d5351d1-4609-7ebf-98cf-f48f7068e183"}},{"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\n\nresult_file = open(\"sift.csv\", \"w\")\nresult_file_obj = csv.writer(result_file)\nresult_file_obj.writerow(np.append(\"id\", species))\n\nfor leaf in test.id:\n    img = cv2.imread(img_path + str(leaf) + \".jpg\")\n    kp, des = sift.detectAndCompute(img, None)\n\n    x = np.zeros(k)\n    nkp = np.size(kp)\n\n    for d in des:\n        idx = kmeans.predict([d])\n        x[idx] += 1/nkp\n\n    res = mlp.predict_proba([x])\n    row = []\n    row.append(leaf)\n\n    for e in res[0]:\n        row.append(e)\n\n    result_file_obj.writerow(row)\n\nresult_file.close()","metadata":{"_cell_guid":"58247f35-2213-d774-4229-48b308c62613"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Alternative ##\nI also run this script with ORB instead of SIFT and I got best results. To do it, just replace `cv2.xfeatures2d.SIFT_create()` by `cv2.ORB_create()`.","metadata":{"_cell_guid":"d10668e0-7c50-1f79-1cb4-bf4d1b87977b"}}]}