{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network (CNN)\n",
        "\n",
        "##### In this code it is implemented an artificial neural network for a multiclass classification task.\n",
        "##### In particular it is used pytorch and to extract the features from the images are implemented convolutional layers\n"
      ],
      "metadata": {
        "id": "bJCfY7yaZKvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyABINiYu71K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gPJ5JtM6Qtx",
        "outputId": "fe1c66bc-290a-4fa8-9601-b16d59032be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mZ5J0VCisBtk"
      },
      "outputs": [],
      "source": [
        "#extract\n",
        "!tar -xf /content/drive/MyDrive/Supervised_Learning/Exam/annot.tar\n",
        "!tar -xf /content/drive/MyDrive/Supervised_Learning/Exam/train.tar\n",
        "!tar -xf /content/drive/MyDrive/Supervised_Learning/Exam/val.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv9s0S9yB4Z5"
      },
      "outputs": [],
      "source": [
        "# Select the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Select the batch size\n",
        "bs = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dOiyrBd85S3"
      },
      "outputs": [],
      "source": [
        "# Initialize the paths of the folders in which the images are stored\n",
        "train_dir = '/content/train_set'\n",
        "test_dir = '/content/val_set'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmRo2_MCu6Ui"
      },
      "outputs": [],
      "source": [
        "class FoodDataset(Dataset):\n",
        "  def __init__(self, root, df, split=None, transform=None):\n",
        "    self.root = root\n",
        "    self.split = split\n",
        "    self.transform = transform\n",
        "    self.df = df\n",
        "\n",
        "    # Compute the length of the dataset\n",
        "    total_data_len = len(df)\n",
        "\n",
        "    # Randomly shuffle indices for train/val/test split\n",
        "    idx = np.arange(total_data_len, dtype=int)\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    # Perform the train/val split (80/20)\n",
        "    if self.split == \"train\":\n",
        "        self.im_dir = [os.path.join(self.root, self.df.iloc[i, 0]) for i in idx[:int(total_data_len * 0.8)]]\n",
        "        self.target = [self.df.iloc[i, 1] for i in idx[:int(total_data_len * 0.8)]]\n",
        "    elif self.split == \"val\":\n",
        "        self.im_dir = [os.path.join(self.root, self.df.iloc[i, 0]) for i in idx[int(total_data_len * 0.8):]]\n",
        "        self.target = [self.df.iloc[i, 1] for i in idx[int(total_data_len * 0.8):]]\n",
        "    # Take all the images in the test folder\n",
        "    elif self.split == \"test\":\n",
        "        self.im_dir = [os.path.join(self.root, self.df.iloc[i, 0]) for i in idx[:]]\n",
        "        self.target = [self.df.iloc[i, 1] for i in idx[:]]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    image = Image.open(self.im_dir[idx])\n",
        "    label = self.target[idx]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.im_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhSVZ3U5u6SL"
      },
      "outputs": [],
      "source": [
        "# Initialize the dataset\n",
        "df=pd.read_csv('/content/train_info.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj9Jc5xJu6Oz"
      },
      "outputs": [],
      "source": [
        "file = '/content/class_list.txt'\n",
        "\n",
        "# open the file in read mode\n",
        "with open(file, 'r') as file:\n",
        "    # read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "# initialize an empty dictionary\n",
        "classes = {}\n",
        "\n",
        "# iterate through each line and split key-value pairs\n",
        "for line in lines:\n",
        "    key, value = line.strip().split(' ')\n",
        "    classes[key.strip()] = value.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STNBrl5PLNsd"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally for training augmentation\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation((0,180)),  # Rotate images randomly by 0 to 180 degrees\n",
        "    transforms.RandomPerspective(),  # Apply random perspective transformation\n",
        "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), ),  # Apply random affine transformation\n",
        "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
        "    transforms.Normalize(  # Normalize pixel values based on ImageNet statistics\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images to 256x256 (consistent with training)\n",
        "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
        "    transforms.Normalize(  # Normalize pixel values using the same statistics\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LUHIH3KNkL7"
      },
      "outputs": [],
      "source": [
        "# Initialize the train/val dataset\n",
        "train_ds_transformed = FoodDataset(train_dir, df, 'train', transform=transform_train)\n",
        "val_ds_transformed = FoodDataset(train_dir, df, 'val', transform=transform_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82-HJ9asNkJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c94b507-74bd-40c6-e0dc-d6e5c7c0f92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beignet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "         ...,\n",
              "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              "\n",
              "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "         ...,\n",
              "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              "\n",
              "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "         ...,\n",
              "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Select one image to check if the dataset class works properly\n",
        "image, label = train_ds_transformed[103]\n",
        "# Print the label and the image (if the image is a tensor, then it worked)\n",
        "print(classes[str(label)])\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOeQQIcn54pB"
      },
      "outputs": [],
      "source": [
        "# Initialize the train/val data loader\n",
        "train_loader = DataLoader(train_ds_transformed, batch_size=bs, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds_transformed, batch_size=1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3GhiCVr54nC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__() # Call the superclass constructor\n",
        "        self.conv1 = nn.Conv2d(3, 20, 9, stride=3) # First convolutional layer\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Max pooling layer\n",
        "        self.conv2 = nn.Conv2d(20, 40, 9, stride=2) # Second convolutional layer\n",
        "        self.conv3 = nn.Conv2d(40, 80, 3, stride=1, padding=1) # Third convolutional layer\n",
        "        self.fc1 = nn.Linear(80*4*4, 500)  # First fully-connected layer\n",
        "        self.dropout = nn.Dropout(0.25) # Dropout layer\n",
        "        self.fc2 = nn.Linear(500, 346) # Second fully-connected layer\n",
        "        self.fc3 = nn.Linear(346, 251) # Third fully-connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.shape[0],-1) # Flatten the output from convolutional layers\n",
        "        # print(x.shape)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x) # Output layer\n",
        "        return F.log_softmax(x, dim=1) # Apply log softmax to the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVZCDABw54k9",
        "outputId": "be2d654b-d3e2-4444-9a8c-cea64db9d9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully loaded FoodNet\n"
          ]
        }
      ],
      "source": [
        "# Load the network for training or inference (or both)\n",
        "if os.path.exists('/content/FoodNet.pth'):\n",
        "    net = Net()\n",
        "    net.load_state_dict(torch.load('/content/FoodNet.pth', map_location=device))\n",
        "    net.to(device)\n",
        "    print(\"Succesfully loaded FoodNet\")\n",
        "else:\n",
        "    net = Net()\n",
        "    net.to(device)\n",
        "    print(\"FoodNet initialized correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3buMUR9I54jA",
        "outputId": "00d869a2-7f8b-48fe-aa98-68c0c751069c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 20, 83, 83]           4,880\n",
            "         MaxPool2d-2           [-1, 20, 41, 41]               0\n",
            "            Conv2d-3           [-1, 40, 17, 17]          64,840\n",
            "         MaxPool2d-4             [-1, 40, 8, 8]               0\n",
            "            Conv2d-5             [-1, 80, 8, 8]          28,880\n",
            "         MaxPool2d-6             [-1, 80, 4, 4]               0\n",
            "           Dropout-7                 [-1, 1280]               0\n",
            "            Linear-8                  [-1, 500]         640,500\n",
            "           Dropout-9                  [-1, 500]               0\n",
            "           Linear-10                  [-1, 346]         173,346\n",
            "           Linear-11                  [-1, 251]          87,097\n",
            "================================================================\n",
            "Total params: 999,543\n",
            "Trainable params: 999,543\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 1.49\n",
            "Params size (MB): 3.81\n",
            "Estimated Total Size (MB): 6.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "# Print model summary\n",
        "summary(net, (3, 256, 256))  # Input shape (channels, height, width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yehwi7u954g7"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim  # Optimization algorithms for training the model\n",
        "import torch.nn.functional as F  # Common loss functions and activation functions\n",
        "import itertools  # Utility functions for generating combinations\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR  # Learning rate scheduler for training\n",
        "import matplotlib.pyplot as plt  # Plotting library for visualization\n",
        "\n",
        "\n",
        "# Define training parameters (epochs, loss function, optimizer, and scheduler)\n",
        "epochs = 100  # Number of training epochs\n",
        "criterion = nn.CrossEntropyLoss()  # Crossentropyloss function for classification\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)  # Adam optimizer with learning rate 0.001\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max=len(train_loader) * epochs,  # Maximum number of iterations for scheduler\n",
        "                              eta_min=1e-8)  # Minimum learning rate for scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "collapsed": true,
        "id": "_3sMSJp1HXSl",
        "outputId": "138c1afa-04e5-43e7-e4d4-cb355dde324f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ad622ea72c74>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Append ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWELCOME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAuthenticationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'digest sent was rejected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf') # Initialize the validation loss\n",
        "patience = 10 # Initialize the patience level\n",
        "patience_counter = 0 # Initialize the counter\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Initialize Running loss, ground truth labels and predictions\n",
        "    running_loss = []\n",
        "    gt = []\n",
        "    pred = []\n",
        "\n",
        "    # Set the network in training mode\n",
        "    net.train()\n",
        "\n",
        "    for i, inp in enumerate(train_loader):\n",
        "        inputs, labels = inp\n",
        "        gt.append(labels.cpu().numpy())   # Append ground truth\n",
        "\n",
        "        inputs = inputs.to(device)  # Move data to the appropriate device (CPU or GPU)\n",
        "        labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "        # Zero the parameter gradients before each backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass, calculate loss\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        pred.append(torch.max(outputs.cpu(), 1)[1])\n",
        "\n",
        "        # Backward pass and parameter update\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "        optimizer.step()  # Update model weights based on gradients\n",
        "        scheduler.step()  # Update learning rate according to the scheduler\n",
        "\n",
        "        # Print statistics (every 10% of the training data)\n",
        "        running_loss.append(loss.item())\n",
        "        if (i + 1) % (len(train_loader) // 10) == 0:  # Every 10% of the epoch\n",
        "            gt = np.stack(list(itertools.chain.from_iterable(gt))).squeeze()  # Combine ground truth labels\n",
        "            pred = np.stack(list(itertools.chain.from_iterable(pred))).squeeze()  # Combine predictions\n",
        "            acc = np.sum(gt==pred)/len(gt)\n",
        "            print('%d, [%d, %d] loss: %.4f\\tAccuracy: %.3f\\tlr: %.6f' %\n",
        "                      (epoch + 1, i + 1, len(train_loader), np.mean(running_loss), acc, optimizer.param_groups[-1]['lr']))\n",
        "\n",
        "            running_loss = []\n",
        "            gt = []\n",
        "            pred = []\n",
        "\n",
        "    # Validation loop (after each training epoch)\n",
        "    running_loss = []  # List to store validation loss for each batch\n",
        "    gt = []  # List to store ground truth labels (predicted age)\n",
        "    pred = []  # List to store predicted labels (model output)\n",
        "\n",
        "    net.eval()  # Set the model to evaluation mode (deactivates dropout and other training behaviors)\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "        # Get inputs and labels from the data loader\n",
        "        inputs, labels = data\n",
        "        gt.append(labels.item())  # Append ground truth as single values\n",
        "\n",
        "        inputs = inputs.to(device)  # Move data to the appropriate device\n",
        "        labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "        # Forward pass with gradient suppression\n",
        "        with torch.no_grad():\n",
        "            outputs = net(inputs)  # Get model predictions without calculating gradients\n",
        "\n",
        "        pred.append(torch.max(outputs.cpu(), 1)[1])\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "    # Calculate and print validation performance metrics\n",
        "    gt = np.stack(list(gt)).squeeze()\n",
        "    pred = np.stack(list(pred)).squeeze()\n",
        "    acc = np.sum(gt==pred)/len(gt)\n",
        "\n",
        "    print(gt, pred)\n",
        "    print(np.sum(gt==pred),len(gt))\n",
        "\n",
        "    # Compute validation loss\n",
        "    val_loss = np.mean(running_loss)\n",
        "    print('Validation loss: %.6f\\tAccuracy: %.3f' % (val_loss, acc))\n",
        "\n",
        "    # Early stop of the train in case of 10 consecutive non-increasing in validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(net.state_dict(), \"FoodNet.pth\") # Save the best model\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss for {patience} epochs.\")\n",
        "            break\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the test dataframe\n",
        "test_df = pd.read_csv('/content/val_info.csv')"
      ],
      "metadata": {
        "id": "FoKoj4HaFIY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sQwMem354eo"
      },
      "outputs": [],
      "source": [
        "# Initialize the test dataset\n",
        "test_ds_transformed = FoodDataset(test_dir, test_df, 'test', transform=transform_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRW-nhLq54cb"
      },
      "outputs": [],
      "source": [
        "# Initialize the test data loader\n",
        "test_loader = DataLoader(test_ds_transformed, batch_size=1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5aUPgRC54aA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a66195-8a71-47be-f886-179f3618cf92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_thai\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0777,  2.0777,  2.1462,  ...,  2.2147,  2.1975,  2.1804],\n",
              "         [ 2.0948,  2.1119,  2.1462,  ...,  2.2489,  2.2489,  2.2318],\n",
              "         [ 2.1975,  2.1975,  2.2147,  ...,  2.2489,  2.2489,  2.2489],\n",
              "         ...,\n",
              "         [ 0.6221,  0.5707,  0.5193,  ...,  0.5364,  0.6392,  0.5364],\n",
              "         [ 0.6563,  0.6221,  0.5878,  ...,  0.5193,  0.6563,  0.5022],\n",
              "         [ 0.7591,  0.7248,  0.6563,  ...,  0.4679,  0.6221,  0.4337]],\n",
              "\n",
              "        [[ 1.6408,  1.6408,  1.6408,  ...,  1.7283,  1.7283,  1.7108],\n",
              "         [ 1.6057,  1.6057,  1.5882,  ...,  1.7808,  1.7808,  1.7633],\n",
              "         [ 1.6232,  1.6232,  1.6057,  ...,  1.8333,  1.8158,  1.8158],\n",
              "         ...,\n",
              "         [ 0.0126, -0.0049, -0.0749,  ...,  0.2227,  0.3627,  0.2927],\n",
              "         [ 0.0826,  0.0476, -0.0049,  ...,  0.2052,  0.3803,  0.2577],\n",
              "         [ 0.1877,  0.1527,  0.1176,  ...,  0.1877,  0.3452,  0.1877]],\n",
              "\n",
              "        [[ 1.1934,  1.1934,  1.1759,  ...,  1.2631,  1.2108,  1.1934],\n",
              "         [ 1.1411,  1.1411,  1.1062,  ...,  1.3154,  1.2631,  1.2457],\n",
              "         [ 1.1062,  1.1062,  1.0888,  ...,  1.3502,  1.3328,  1.3328],\n",
              "         ...,\n",
              "         [-0.5147, -0.5495, -0.5844,  ..., -0.2010, -0.0441, -0.1312],\n",
              "         [-0.4624, -0.4973, -0.5147,  ..., -0.2184, -0.0267, -0.1661],\n",
              "         [-0.3578, -0.3927, -0.4101,  ..., -0.2532, -0.0615, -0.2358]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Check if the image is correct\n",
        "image, label = test_ds_transformed[103]\n",
        "\n",
        "print(classes[str(label)])\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "running_loss = []  # List to store test loss for each batch\n",
        "gt_labels = []  # List to store ground truth labels\n",
        "pr_labels = []  # List to store predicted labels\n",
        "top5_correct = 0  # Counter for top-5 accuracy\n",
        "\n",
        "net.eval()  # Set the model to evaluation mode\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    # Get inputs and labels from the data loader\n",
        "    inputs, labels = data\n",
        "    gt_labels.append(labels.item())  # Append ground truth as single values\n",
        "\n",
        "    inputs = inputs.to(device)  # Move data to the appropriate device\n",
        "    labels = labels.to(device)  # Move labels to the appropriate device\n",
        "\n",
        "    # Forward pass with gradient suppression\n",
        "    with torch.no_grad():\n",
        "        outputs = net(inputs)  # Get model predictions without calculating gradients\n",
        "\n",
        "    pr_labels.append(torch.max(outputs.cpu(), 1)[1])\n",
        "    loss = criterion(outputs, labels)\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "    # Calculate top-5 accuracy\n",
        "    top5_preds = torch.topk(outputs, 5, dim=1).indices.cpu().numpy()\n",
        "    if labels.cpu().numpy() in top5_preds:\n",
        "        top5_correct += 1\n",
        "\n",
        "# Calculate and print validation performance metrics\n",
        "gt = np.stack(list(gt_labels)).squeeze()\n",
        "pred = np.stack(list(pr_labels)).squeeze()\n",
        "test_loss = np.mean(running_loss)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "acc = accuracy_score(gt, pred)\n",
        "prec = precision_score(gt, pred, average='macro')\n",
        "rec = recall_score(gt, pred, average='macro')\n",
        "top5_acc = top5_correct / len(test_loader.dataset)\n",
        "\n",
        "# Display the outcomes\n",
        "print(f'Test loss: {test_loss:.6f}\\t Accuracy: {acc:.3f}\\tPrecision: {prec:.3f}\\tRecall: {rec:.3f}\\tTop-5 Accuracy: {top5_acc:.3f}')\n"
      ],
      "metadata": {
        "id": "KWRKlGRIIcn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7211d1e-e63f-40f2-ca01-c0294865ba2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[180 176  64 ...  58 209   6] [168 118 172 ...  34  69 218]\n",
            "2769 11993\n",
            "Test loss: 3.461395\tAccuracy: 0.231\n",
            "Test loss: 3.461395\t Accuracy: 0.231\tPrecision: 0.221\tRecall: 0.222\tTop-5 Accuracy: 0.490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FygawYr954Xk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gcQECDL6jEa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0tuPSZtA0J8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}